{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import ssl\n",
    "import nltk\n",
    "import requests\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from ibm_watson_machine_learning.foundation_models import ModelInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from PDF using an API\n",
    "\n",
    "def extract_data_from_pdf(input_doc):\n",
    "    # Set the URL for the API\n",
    "    url = \"http://xxx.com/api/v1/task/process\"\n",
    "    print(\"URL being accessed:\", url)  # Debug: print the URL being accessed\n",
    "\n",
    "    # Headers\n",
    "    headers = {'accept': 'application/octet-stream'}\n",
    "\n",
    "    # Parameters and payload setup\n",
    "    files = {\n",
    "        'parameters': (None, '{}', 'application/json'),\n",
    "        'model_id': (None, 'processor', 'application/json'),\n",
    "        'inputs.file': (input_doc, open(input_doc, 'rb'), 'application/pdf')\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    files['inputs.file'][1].close()  # Ensure the file is closed after sending\n",
    "\n",
    "    # Check if the response was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to process document: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "    # Check if the content type is 'application/zip'\n",
    "    if 'application/zip' not in response.headers.get('Content-Type', ''):\n",
    "        print(\"Received incorrect file type, expected application/zip\")\n",
    "        return None\n",
    "\n",
    "    # Output path for the results\n",
    "    output_path = f\"{input_doc[:-4]}_results.zip\"\n",
    "\n",
    "    # Write the response content to a file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Function to unzip a file\n",
    "def unzip_file(zip_file_path, extract_to_folder):\n",
    "    if zip_file_path is not None:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            print(f\"unzip folder:  {extract_to_folder}\")\n",
    "            zip_ref.extractall(extract_to_folder)\n",
    "    else:\n",
    "        print(\"No zip file provided to unzip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the information from pdf and saving to output.json\n",
    "#input_pdf_file = \"NYSE_DDS_2023.pdf\"\n",
    "input_pdf_file =\"IBM_Annual_Report_2022.pdf\"\n",
    "ziped_pdf_data = extract_data_from_pdf(input_pdf_file)\n",
    "extract_to_folder = ''\n",
    "if ziped_pdf_data:\n",
    "    unzip_file(ziped_pdf_data, extract_to_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Extraction from JSON\n",
    "\n",
    "def add_newline_after_colon_and_number(input_string):\n",
    "    \"\"\"\n",
    "    Add a newline after a colon followed by a number in the input string.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): Input string to process.\n",
    "\n",
    "    Returns:\n",
    "    str: Processed string with newline after colon-number pairs.\n",
    "    \"\"\"\n",
    "    output_string = \"\"\n",
    "    i = 0\n",
    "    while i < len(input_string):\n",
    "        char = input_string[i]\n",
    "        if char == \":\":\n",
    "            # Look for the end of the number\n",
    "            j = i + 1\n",
    "            while j < len(input_string) and (input_string[j].isdigit() or input_string[j] == \"-\" or input_string[j] == \" \"):\n",
    "                j += 1\n",
    "            # Add the colon and number together\n",
    "            output_string += input_string[i:j].strip() + \" \\n\"\n",
    "            i = j\n",
    "        else:\n",
    "            output_string += char\n",
    "            i += 1\n",
    "    return output_string.strip() + \"\\n\"\n",
    "\n",
    "def extract_text_from_page_details(page_details):\n",
    "    \"\"\"\n",
    "    Extract text from the page details.\n",
    "\n",
    "    Args:\n",
    "    page_details (dict): Details of a page containing text tokens.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the page.\n",
    "    \"\"\"\n",
    "    # Use list comprehension to extract text from tokens\n",
    "    tokens = page_details.get(\"tokens\", [])\n",
    "    extracted_text = \" \".join(token.get(\"text\", \"\") for token in tokens)\n",
    "    extracted_text = add_newline_after_colon_and_number(extracted_text)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def process_pages_and_structure_text(pages_list):\n",
    "    \"\"\"\n",
    "    Process the list of pages to extract and structure text.\n",
    "\n",
    "    Args:\n",
    "    pages_list (list): List of page details.\n",
    "\n",
    "    Returns:\n",
    "    list: List of extracted and structured texts for each page.\n",
    "    \"\"\"\n",
    "    extracted_texts = []\n",
    "    for page_details in pages_list:\n",
    "        page_info = extract_text_from_page_details(page_details)\n",
    "        # Join sentences with \"\\n\" for readability\n",
    "        page_info_string = \"\\n\".join(nltk.sent_tokenize(page_info))\n",
    "        extracted_texts.append(page_info_string)\n",
    "    return extracted_texts\n",
    "\n",
    "# Load JSON data from the \"output.json\" file\n",
    "with open(\"output.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract pages list\n",
    "pages_list = data.get(\"pages\", [])\n",
    "\n",
    "# Process pages\n",
    "extracted_texts = process_pages_and_structure_text(pages_list)\n",
    "\n",
    "# Open the file in write mode and save the extracted texts\n",
    "output_file = 'extracted_texts.txt'\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    for i, text in enumerate(extracted_texts, start=1):\n",
    "        file.write(f\"Page {i}:\\n\")\n",
    "        file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "print(\"Extracted texts have been saved to 'extracted_texts.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for creating index and chunks\n",
    "\n",
    "def create_index(es_client, index_name):\n",
    "    index_name = index_name\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 384  # Dimensionality of the model's embeddings\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if es_client.indices.exists(index=index_name):\n",
    "        logging.info(f\"Deleting existing index: {index_name}\")\n",
    "        es_client.indices.delete(index=index_name)\n",
    "    logging.info(f\"Creating index: {index_name}\")\n",
    "    es_client.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    logging.info(f\"Loading text file: {file_path}\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=100):\n",
    "    logging.info(f\"Chunking text of length {len(text)}\")\n",
    "    words = text.split()\n",
    "    if len(words) <= chunk_size:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk))\n",
    "    #logging.info(f\"Created chunks: {chunks}\")\n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(text_chunks):\n",
    "    logging.info(f\"Generating embeddings for {len(text_chunks)} chunks.\")\n",
    "    embeddings = model.encode(text_chunks)\n",
    "    #logging.info(f\"Generated embeddings: {embeddings}\")\n",
    "    return embeddings\n",
    "\n",
    "def store_embeddings_in_elasticsearch(index_name, embeddings, text_chunks, file_key):\n",
    "    for idx, (embedding, chunk) in enumerate(zip(embeddings, text_chunks)):\n",
    "        document = {\n",
    "            \"embedding\": embedding.tolist(),\n",
    "            \"text\": chunk  # Store actual text chunk here\n",
    "        }\n",
    "        es.index(index=index_name, id=f\"{file_key}-{idx}\", document=document)\n",
    "        #logging.info(f\"Stored embedding for chunk {idx} of {file_key} in Elasticsearch\")\n",
    "\n",
    "def create_chunks(file_path):\n",
    "    text = load_text_file(file_path)\n",
    "    if text and text.strip():  # Check if text is not empty\n",
    "        text_chunks = chunk_text(text, chunk_size=100, overlap=50)\n",
    "        embeddings = generate_embeddings(text_chunks)\n",
    "        store_embeddings_in_elasticsearch(index_name, embeddings, text_chunks, \"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Chunking and Embedding\n",
    "\n",
    "# Ensure the NLTK data is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['ELASTIC_HOST'] = 'xxx'\n",
    "os.environ['ELASTIC_PORT'] = '31066'\n",
    "os.environ['ELASTIC_USERNAME'] = 'xxx'\n",
    "os.environ['ELASTIC_PASSWORD'] = 'xxx'\n",
    "os.environ['ELASTIC_TLS_CERT'] = 'certificate.crt'\n",
    "\n",
    "# Elasticsearch credentials\n",
    "ELASTIC_HOST = os.getenv(\"ELASTIC_HOST\")\n",
    "ELASTIC_PORT = os.getenv(\"ELASTIC_PORT\")\n",
    "ELASTIC_USERNAME = os.getenv(\"ELASTIC_USERNAME\")\n",
    "ELASTIC_PASSWORD = os.getenv(\"ELASTIC_PASSWORD\")\n",
    "ELASTIC_TLS_CERT = os.getenv(\"ELASTIC_TLS_CERT\")\n",
    "\n",
    "# Create an SSL context for Elasticsearch\n",
    "ssl_context = ssl.create_default_context(cafile=ELASTIC_TLS_CERT)\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Initialize the Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    [f\"https://{ELASTIC_USERNAME}:{ELASTIC_PASSWORD}@{ELASTIC_HOST}:{ELASTIC_PORT}\"],\n",
    "    ssl_context=ssl_context\n",
    ")\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "logging.info(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Specify the single text file path\n",
    "file_path = \"extracted_texts.txt\"\n",
    "\n",
    "# Run the main function with the specified text file\n",
    "index_name = \"embeddings_pdf\"\n",
    "create_index(es, index_name)\n",
    "\n",
    "create_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "META_PROMPT_EN = \"\"\"<|system|>\n",
    "You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\n",
    "<|user|>\n",
    "\n",
    "You are an AI powered assistant who has access to vector database.\n",
    "You should help by providing accurate answers to questions.\n",
    "You answer must only answer the question asked.\n",
    "If you don't know the answer to a question, DO NOT share false information. Instead, say that you don't know the answer.\n",
    "\n",
    "You are given the following information from knowledge base that should be your only source of information to answer the question.\n",
    "\n",
    "[Document]\n",
    "\n",
    "{context_str}\n",
    "\n",
    "[End]\n",
    "\n",
    "{query_str}\n",
    "\n",
    "<|assistant|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for getting prompte template and querying LLM\n",
    "\n",
    "def get_prompt(context_str, query_str):\n",
    "    \"\"\"\n",
    "    Fill the META_PROMPT_EN template with the given context and query.\n",
    "\n",
    "    Args:\n",
    "        context_str (str): The context string extracted from the knowledge base.\n",
    "        query_str (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The filled prompt ready to be used with the language model.\n",
    "    \"\"\"\n",
    "    return META_PROMPT_EN.format(context_str=context_str, query_str=query_str)\n",
    "\n",
    "def query_model(model, query):\n",
    "    if query in cache:\n",
    "        return cache[query]\n",
    "    \n",
    "    response = model.generate(query)  # Pass only the query text\n",
    "    generated_text = response.get('results', [{}])[0].get('generated_text', '')\n",
    "    cache[query] = generated_text\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def search_single_embeddings(index_name, query_embedding, top_k=3):\n",
    "    logging.info(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "    response = es.search(\n",
    "        index=indexname,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_embedding.tolist()}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": top_k,\n",
    "            \"_source\": [\"text\"]\n",
    "        }\n",
    "    )\n",
    "    logging.info(f\"Elasticsearch response: {response}\")\n",
    "    return response\n",
    "\n",
    "def search_similar_embeddings(index_list, query_embedding, top_k=3):\n",
    "    logging.info(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "    # Specify multiple indices separated by commas\n",
    "    indices = index_list  # Adjust this to your actual index names\n",
    "    response = es.search(\n",
    "        index=indices,  # Use the indices variable\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_embedding.tolist()}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": top_k,\n",
    "            \"_source\": [\"text\"]\n",
    "        }\n",
    "    )\n",
    "    logging.info(f\"Elasticsearch response: {response}\")\n",
    "    return response\n",
    "\n",
    "def run_rag_query(index_list, query):\n",
    "    query_embedding = sentence_model.encode([query])[0]\n",
    "    #search_result = search_single_embeddings(query_embedding, top_k=3)\n",
    "    search_result = search_similar_embeddings(index_list, query_embedding, top_k=3)\n",
    "\n",
    "    try:\n",
    "        hits = search_result['hits']['hits']\n",
    "        relevant_chunks = [' '.join(hit['_source']['text'].split()) for hit in hits]\n",
    "        context_str = \"\\n\\n\".join(relevant_chunks)\n",
    "        logging.info(\"Here are the most relevant chunks: %s\", relevant_chunks)\n",
    "    except (KeyError, IndexError) as e:\n",
    "        logging.error(f\"Error accessing document content: {e}\")\n",
    "        return {'error': 'Error processing retrieved document'}\n",
    "\n",
    "    # Constructing the prompt with context\n",
    "    prompt = get_prompt(context_str, query)\n",
    "    logging.info(f\"Generated prompt: {prompt}\")\n",
    "\n",
    "    # Generate response using the model\n",
    "    generated_text = query_model(watson_model, prompt)\n",
    "    formatted_response = f\"Query: {query}\\nGenerated Response: {generated_text}\"\n",
    "    logging.info(f\"Final formatted response: {formatted_response}\")\n",
    "\n",
    "    return {'response': generated_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Processing and Semantic Search and Integration with Language Model (LLM)\n",
    "\n",
    "\n",
    "# Set environment variables for LLM\n",
    "os.environ['MODEL_ID'] = 'ibm/granite-13b-chat-v2'\n",
    "os.environ['API_KEY'] = 'xxx'\n",
    "os.environ['URL'] = 'xxx'\n",
    "os.environ['PROJECT_ID'] = 'xxx\n",
    "# IBM Watson model configuration\n",
    "generate_params = {\n",
    "    'max_new_tokens': 1000  # Adjust based on model's limit and desired output length\n",
    "}\n",
    "\n",
    "# Initialize the IBM Watson Machine Learning model\n",
    "watson_model = ModelInference(\n",
    "    model_id=os.getenv('MODEL_ID'),\n",
    "    credentials={\"apikey\": os.getenv('API_KEY'), \"url\": os.getenv('URL')},\n",
    "    project_id=os.getenv('PROJECT_ID'),\n",
    "    params=generate_params\n",
    ")\n",
    "\n",
    "# Simple cache to store responses\n",
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "query = \"What is ...?\"\n",
    "index_list=\"embeddings_xxx\"\n",
    "result = run_rag_query(index_list, query)\n",
    "print(result['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
